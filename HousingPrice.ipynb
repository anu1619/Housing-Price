{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5f25e2e",
   "metadata": {},
   "source": [
    "# Advance Linear Regression\n",
    "## Shared Bikes Demand Prediction - Assignment Solution\n",
    "\n",
    "#### Problem Statement:\n",
    "\n",
    "A US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia.\n",
    "\n",
    "The company is looking at prospective properties to buy to enter the market. You are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\n",
    "\n",
    "Essentially, the company wants to know â€”\n",
    "\n",
    "\n",
    "- Which variables are significant in predicting the price of a house.\n",
    "\n",
    "\n",
    "- How well those variables describe the price of a house.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f21f205",
   "metadata": {},
   "source": [
    "The solution is divided into the following sections: \n",
    "- Data understanding and exploration\n",
    "- Data Visualisation \n",
    "- Data preparation\n",
    "- Model building and evaluation\n",
    "- Subjective question solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc78bd6",
   "metadata": {},
   "source": [
    "### 1. Data Understanding and Exploration\n",
    "\n",
    "Let's first import the required libraries and have a look at the dataset and understand the size, attribute names etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9403fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48147507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Let's look at the few features \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae81e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting insights of the features\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb8dd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of the dataset: 1460 rows, 81 columns, many features have null values\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20ee20b",
   "metadata": {},
   "source": [
    "#### Understanding the Data Dictionary and parts of Data Preparation\n",
    "\n",
    "The data dictionary contains the meaning of various attributes; some of which are explored and manipulated here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d12b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the above stats we can say that there are columns with more than 80% null values, therefore, let's drop those columns\n",
    "df1 = df.drop(df.columns[(df.isnull().sum()/len(df.index))>0.80], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66787ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05577dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop few more columns\n",
    "# MasVnrType has more than 59% rows empty\n",
    "# FireplaceQu has more than 47% rows empty\n",
    "# Id column has no influence on target variable i.e Sale Price\n",
    "\n",
    "df1 = df1.drop([\"MasVnrType\",\"FireplaceQu\", \"Id\"], axis=1)\n",
    "df1[df1.columns[df1.isnull().sum()>0]].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3619b6",
   "metadata": {},
   "source": [
    "There are `NA` rows in few feature which represents some meaning, therefore, instead of dropping the null values let's convert columns to meaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bd4d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `NA` in basement related features represents no basement, let's replace `NA` rows by `Nb`\n",
    "\n",
    "df1[\"BsmtQual\"] = df1['BsmtQual'].apply(lambda x: x if isinstance(x,str) else \"Nb\")\n",
    "df1['BsmtQual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38bd681",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['BsmtCond'] = df1['BsmtCond'].apply(lambda x: x if isinstance(x,str) else \"Nb\")\n",
    "df1['BsmtCond'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e584b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['BsmtExposure'] = df1['BsmtExposure'].apply(lambda x: x if isinstance(x,str) else \"Nb\")\n",
    "df1['BsmtExposure'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb0a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['BsmtFinType1'] = df1['BsmtFinType1'].apply(lambda x: x if isinstance(x,str) else \"Nb\")\n",
    "df1['BsmtFinType1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f15d0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['BsmtFinType2'] = df1['BsmtFinType2'].apply(lambda x: x if isinstance(x,str) else \"Nb\")\n",
    "df1['BsmtFinType2'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f22498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# `NA` in basement related features represents no basement, let's replace `NA` rows by `Nb`\n",
    "df1['GarageType'] = df1['GarageType'].apply(lambda x: x if isinstance(x,str) else \"NG\")\n",
    "df1['GarageType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5673cefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['GarageFinish'] = df1['GarageFinish'].apply(lambda x: x if isinstance(x,str) else \"NG\")\n",
    "df1['GarageFinish'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['GarageQual'] = df1['GarageQual'].apply(lambda x: x if isinstance(x,str) else \"NG\")\n",
    "df1['GarageQual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca76ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['GarageCond'] = df1['GarageCond'].apply(lambda x: x if isinstance(x,str) else \"NG\")\n",
    "df1['GarageCond'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea40434",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(\"GarageYrBlt\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b950e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# listing down all features have null values\n",
    "df1[df1.columns[df1.isnull().sum()>0]].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff8fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's drop rows containing null values for MasVnrArea and Electrical\n",
    "df1 = df1.drop(df1.index[df1['MasVnrArea'].isnull()].tolist(), axis=0)\n",
    "df1[df1.columns[df1.isnull().sum()>0]].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff46537",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop(df1.index[df1['Electrical'].isnull()].tolist(), axis=0)\n",
    "df1[df1.columns[df1.isnull().sum()>0]].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be4e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing LotFrontage null values by it's mode\n",
    "df1['LotFrontage'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42feae1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[df1[\"LotFrontage\"].isnull(), \"LotFrontage\"] = 60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8f4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a31ee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning string values to different months instead of numeric values which may misindicate some order to it.\n",
    "# A function has been created to map the actual numbers to categorical levels.\n",
    "def object_map(x):\n",
    "    return x.map({1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun', 7: 'Jul',8: 'Aug',9: 'Sept',10: 'Oct',11: 'Nov',12: 'Dec'})\n",
    "\n",
    "# Applying the function to the two columns\n",
    "df1[['MoSold']] = df1[['MoSold']].apply(object_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a6b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['MoSold'].astype('category').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['YrSold'].astype('category').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4e1c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All categorical variables in the dataset\n",
    "df1_categorical=df1.select_dtypes(exclude=['float64','datetime64','int64'])\n",
    "print(df1_categorical.columns)\n",
    "print(len(df1_categorical.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efc0250",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 50))  \n",
    "plt.subplot(13,3,1)\n",
    "sns.boxplot(x = 'MSZoning', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,2)\n",
    "sns.boxplot(x = 'Street', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,3)\n",
    "sns.boxplot(x = 'LotShape', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,4)\n",
    "sns.boxplot(x = 'LandContour', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,5)\n",
    "sns.boxplot(x = 'Utilities', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,6)\n",
    "sns.boxplot(x = 'LotConfig', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,7)\n",
    "sns.boxplot(x = 'LandSlope', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,8)\n",
    "sns.boxplot(x = 'Neighborhood', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,9)\n",
    "sns.boxplot(x = 'Condition1', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,10)\n",
    "sns.boxplot(x = 'Condition2', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,11)\n",
    "sns.boxplot(x = 'BldgType', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,12)\n",
    "sns.boxplot(x = 'HouseStyle', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,13)\n",
    "sns.boxplot(x = 'RoofStyle', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,14)\n",
    "sns.boxplot(x = 'RoofMatl', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,15)\n",
    "sns.boxplot(x = 'Exterior1st', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,16)\n",
    "sns.boxplot(x = 'Exterior2nd', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,17)\n",
    "sns.boxplot(x = 'ExterQual', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,18)\n",
    "sns.boxplot(x = 'ExterCond', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,19)\n",
    "sns.boxplot(x = 'Foundation', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,20)\n",
    "sns.boxplot(x = 'BsmtQual', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,21)\n",
    "sns.boxplot(x = 'BsmtCond', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,22)\n",
    "sns.boxplot(x = 'BsmtExposure', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,23)\n",
    "sns.boxplot(x = 'BsmtFinType1', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,24)\n",
    "sns.boxplot(x = 'BsmtFinType2', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,25)\n",
    "sns.boxplot(x = 'Heating', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,26)\n",
    "sns.boxplot(x = 'HeatingQC', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,27)\n",
    "sns.boxplot(x = 'CentralAir', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,28)\n",
    "sns.boxplot(x = 'Electrical', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,29)\n",
    "sns.boxplot(x = 'KitchenQual', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,30)\n",
    "sns.boxplot(x = 'Functional', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,31)\n",
    "sns.boxplot(x = 'GarageType', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,33)\n",
    "sns.boxplot(x = 'GarageFinish', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,34)\n",
    "sns.boxplot(x = 'GarageQual', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,35)\n",
    "sns.boxplot(x = 'GarageCond', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,36)\n",
    "sns.boxplot(x = 'PavedDrive', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,37)\n",
    "sns.boxplot(x = 'MoSold', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,38)\n",
    "sns.boxplot(x = 'SaleType', y = 'SalePrice', data = df1)\n",
    "plt.subplot(13,3,39)\n",
    "sns.boxplot(x = 'SaleCondition', y = 'SalePrice', data = df1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a31f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop([\"LandSlope\", \"Exterior2nd\", \"BsmtFinType2\", 'Condition2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa3511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All numeric variables in the dataset\n",
    "df1_numeric = df1.select_dtypes(include=['float64', 'int64'])\n",
    "df1_numeric.pop('SalePrice')\n",
    "df1_numeric.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d2bf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 30))  \n",
    "plt.subplot(3,2,1)\n",
    "col = df1_numeric.columns[:7]\n",
    "col = col.insert(0,\"SalePrice\")\n",
    "sns.heatmap(df1[col].corr(), annot=True)\n",
    "plt.subplot(3,2,2)\n",
    "col = df1_numeric.columns[7:14]\n",
    "col = col.insert(0,\"SalePrice\")\n",
    "sns.heatmap(df1[col].corr(), annot=True)\n",
    "plt.subplot(3,2,3)\n",
    "col = df1_numeric.columns[14:21]\n",
    "col = col.insert(0,\"SalePrice\")\n",
    "sns.heatmap(df1[col].corr(), annot=True)\n",
    "plt.subplot(3,2,4)\n",
    "col = df1_numeric.columns[21:28]\n",
    "col = col.insert(0,\"SalePrice\")\n",
    "sns.heatmap(df1[col].corr(), annot=True)\n",
    "plt.subplot(3,2,5)\n",
    "col = df1_numeric.columns[28:]\n",
    "col = col.insert(0,\"SalePrice\")\n",
    "sns.heatmap(df1[col].corr(), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4192b7d7",
   "metadata": {},
   "source": [
    "The heatmap shows some useful insights:\n",
    "\n",
    "Correlation of SalePrice with independent variables:\n",
    "- SalePrice is highly (positively) correlated with 'OverallQual' and 'GrLivArea' and further it is high with 'GarageCars' 'GarageArea', 'TotalRmsAbvGrd', 'YearBuilt', 'YearRemodAdd', 'TotalBsmtSF', '1stFlrSF', 'MasvnrArea', 'fullbath'\n",
    "\n",
    "- SalePrice is negatively correlated with no feature.\n",
    "\n",
    "Correlation among independent variables:\n",
    "- Some of the independent variables are highly correlated (look at the top-left part of matrix): 'OverallQual', 'YearBuilt' and 'YearRemodAdd' are highly (positively) correlated. The correlation between the three. '1stFlrSF' and 'TotalBsmtSF' also high correlation, almost equal to 1. Further we can see there many feature with highly correlating with each other.\n",
    "\n",
    "\n",
    "Thus, while building the model, we'll have to pay attention to multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a54d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.drop([\"YearBuilt\",\"YearRemodAdd\", \"1stFlrSF\", \"BsmtFinSF1\", \"MasVnrArea\", \"BsmtUnfSF\", \"FullBath\", \"HalfBath\", \"GarageArea\",\"TotRmsAbvGrd\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03a2f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f265e4a9",
   "metadata": {},
   "source": [
    "## 3. Data Preparation \n",
    "\n",
    "\n",
    "#### Data Preparation\n",
    "\n",
    "Let's now prepare the data and build the model.\n",
    "Note that we had not included 'yr', 'mnth', 'holiday', 'weekday' and 'workingday' as object variables in the initial data exploration steps so as to avoid too many dummy variables creation. They have binary values: 0s and 1s in them which have specific meanings associated with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603b75b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset all categorical variables\n",
    "df1_categorical=df1.select_dtypes(include=['object'])\n",
    "df1_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e760d2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert into dummies\n",
    "df1_dummies = pd.get_dummies(df1_categorical, drop_first=True)\n",
    "df1_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0030b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop categorical variable columns\n",
    "df1 = df1.drop(list(df1_categorical.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c6c784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dummy variables with the original dataframe\n",
    "df1 = pd.concat([df1, df1_dummies], axis=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcc874b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23560571",
   "metadata": {},
   "source": [
    "## 4. Model Building and Evaluation\n",
    "\n",
    "Let's start building the model. The first step to model building is the usual test-train split. So let's perform that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dfc64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(0)\n",
    "df_train, df_test= train_test_split(df1, train_size = 0.7, test_size = 0.3, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedb3043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf23cf3",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "Now that we have done the test-train split, we need to scale the variables for better interpretability. But we only need the scale the numeric columns and not the dummy variables. Let's take a look at the list of numeric variables we had created in the beginning. Also, the scaling has to be done only on the train dataset as you don't want it to learn anything from the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1d9965",
   "metadata": {},
   "source": [
    "Let's scale all these columns using MinMaxScaler. You can use any other scaling method as well; it is totally up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e16e03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18edc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\n",
    "var = df_train.select_dtypes(include=['float64', 'int64'])\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69002728",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[var.columns] = scaler.fit_transform(df_train[var.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67427efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8524f67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.pop(\"SalePrice\")\n",
    "X_train = df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ca7e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[var.columns] = scaler.transform(df_test[var.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bb234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = df_test.pop(\"SalePrice\")\n",
    "X_test = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20aeadba",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression() \n",
    "reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb5f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the basis of the model\n",
    "y_pred = reg.predict(X_train)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93384659",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a495c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on the basis of the model\n",
    "y_test_pred = reg.predict(X_test)\n",
    "r2_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb2c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residual Sum of Squares = Mean_Squared_Error * Total number of datapoints\n",
    "rss = np.sum(np.square(y_train - y_pred))\n",
    "print(rss)\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "print(mse)\n",
    "# Root Mean Squared Error\n",
    "rmse = mse**0.5\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f821c50c",
   "metadata": {},
   "source": [
    "# Residual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc1a793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residual graph to see if linearity rule is not violated\n",
    "sns.histplot((y_train-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d57261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Ridge Regression with varying the hyperparameter 'lambda'\n",
    "\n",
    "lambdas = [0,0.0001, 0.001, 0.01, 1, 10, 100] # Higher the value of lambda, \n",
    "                                                  # more the regularization\n",
    "for i in lambdas: # for each lambda we get different model coefficients\n",
    "    ridgereg = Ridge(alpha = i) # Initialize the Ridge Regression model with a specific lambda\n",
    "    ridgereg.fit(X_train, y_train)\n",
    "    print(\"alpha = \" + str(i))\n",
    "    #Computing the r2 score\n",
    "    y_pred = ridgereg.predict(X_train)\n",
    "    print(\"r2 score = \" + str(r2_score(y_train, y_pred))) \n",
    "    y_test_pred = ridgereg.predict(X_test)\n",
    "    print(\"test score = \" + str(r2_score(y_test, y_test_pred)))\n",
    "    predictors = list()\n",
    "    for i, p in enumerate(X_train):\n",
    "        if ridgereg.coef_[i] != 0:\n",
    "            predictors.append(p)\n",
    "    print('no of predictors :' + str(len(predictors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ed3dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Lasso Regression with varying the hyperparameter 'lambda'\n",
    "\n",
    "lambdas = [0,0.0001, 0.001, 0.01, 1, 10, 100] \n",
    "for i in lambdas:\n",
    "    lassoreg = Lasso(alpha = i) # Initialize the lasso Regression model with a specific lambda\n",
    "    lassoreg.fit(X_train, y_train)\n",
    "    # Compute R^2 \n",
    "    print(\"alpha = \" + str(i))\n",
    "    y_pred = lassoreg.predict(X_train)\n",
    "    print(\"r2 score = \" + str(r2_score(y_train, y_pred)))\n",
    "    y_test_pred = lassoreg.predict(X_test)\n",
    "    print(\"test score = \" + str(r2_score(y_test, y_test_pred)))\n",
    "    predictors = list()\n",
    "    for i, p in enumerate(X_train):\n",
    "        if lassoreg.coef_[i] != 0:\n",
    "            predictors.append(p)\n",
    "    print('no of predictors: ' + str(len(predictors)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d3f73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoreg = Lasso(alpha = 0.0001)\n",
    "lassoreg.fit(X_train, y_train)\n",
    "y_pred = lassoreg.predict(X_train)\n",
    "print(\"r2 score = \" + str(r2_score(y_train, y_pred)))\n",
    "y_test_pred = lassoreg.predict(X_test)\n",
    "print(\"test score = \" + str(r2_score(y_test, y_test_pred)))\n",
    "# print(lassoreg.coef_)\n",
    "predictors = dict()\n",
    "coef = list()\n",
    "for i, p in enumerate(X_train):\n",
    "    if lassoreg.coef_[i] != 0:\n",
    "        coef.append(abs(lassoreg.coef_[i]))\n",
    "        predictors[abs(lassoreg.coef_[i])] = p\n",
    "print('no of predictors: ' + str(len(coef)))\n",
    "coef.sort(reverse=True)\n",
    "for c in coef:\n",
    "    print(predictors[c])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
